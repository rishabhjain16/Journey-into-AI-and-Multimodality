# Journey into AI, Multimodality and Related Stuff (Including Datasets)
This repository chronicles the ongoing exploration of multimodal AI and Transformer models, curating summaries of groundbreaking research that integrates various sensory inputs such as vision, sound, and language into cohesive AI systems. It showcases some of the coolest and most exciting work happening in the field, highlighting cutting-edge advancements that blend multiple sensory modalities to expand the capabilities of AI. Additionally, this repository includes my personal notes and insights, making it easier to access and understand the latest research. It’s offers a curated resource for those eager to explore how AI is evolving to process and interact with the world in increasingly human-like ways. Also add-ons to interesting articles, Hugging Face stuff or any datasets.

<!--
Standard Template to Add things: 
| Model Name | Description | Link to Code or Paper | Task |
-->

<!--
Example 
| VALLR | Visual ASR Language Model for Lip Reading | [📁 VALLR](./VALLR/) | Visual Speech Recognition |
-->


## Interesting Papers: 

| Project | Description | Link | TASK |
|---------|-------------|----------|----------------|
| VALLR | Visual ASR Language Model for Lip Reading | [📁 VALLR](./Papers/VALLR/) | Visual Speech Recognition |
| AVFormer | Injecting Vision into Frozen Speech Models for Zero-Shot AV-ASR | [📁 AVFormer](./Papers/AVFormer/) | Audio Video Speech Recognition | 
| HTR-VT | Handwritten Text Recognition with Vision Transformer (uses CTC) | [📁 HTR-VT](./Papers/HTR-VT/) | Handwritten Text Recogntion | 
| AST | Audio Spectrogram Transformer| [📁 AST](./Papers/AST/) | Audio Classification | 
| LAVIS | A Library for Language-Vision Intelligence| [📁 LAVIS](./Papers/LAVIS/) | Language Vision - CLIP, BLIP, X-InstructBLIP, etc.| 
| Qwen2.5 - Omni | An end-to-end multimodal model designed to perceive diverse modalities, including text, images, audio, and video, while simultaneously generating text and natural speech responses in a streaming manner| [📁 Qwen2.5-Omni](./Papers/Omni/) | Multi-Modal LLM, MultiTask| 
| Qwen2-Audio| Large audio-language model (LALM)| [📁 Qwen2-Audio](./Papers/Qwen2-Audio/) | Multi-Modal LLM, Audio Understanding| 
| Qwen2.5-VL| Large vision-language models (LVLMs) | [📁 Qwen2.5-VL](./Papers/Qwen2.5-VL/) | Multi-Modal LLM, Video Understanding| 
| Kimi-Audio | Audio understanding, Generation, and Conversation. | [📁 Kimi-Audio](./Papers/Kimi-Audio/) | ASR, Audio Question Answering (AQA), Automatic Audio Captioning (AAC), Speech emotion Recognition (SER), Sound event/Scene Classification (SEC/ASC), E2E conversations|


## Blogs and Articles:
| Title | Link |
|-------|------|
| Understanding Multimodal LLMs | [📁 Multimodal-LLM](./Blogs/Multi-LLM/) |
| SpeechLMs: LLM-Powered Text-to-Speech and Neural Audio Codecs Explored | [📁 Speech-LMs](./Blogs/Speech-LMs/) |


## Hugging Face Stuff: 
| Project | Description | Link | TASK |
|---------|-------------|------|------|
| Dia-1.6B  | 1.6 billion parameter text-to-speech (TTS) model | [📁 Dia](./HF/Dia/) | TTS |
| smolVLM  | 2B small vision language models  | [📁 smolVLM](./HF/SmolVLM/) | VLM |
| nanoVLM  | training/finetuning small-sized VLMs | [📁 nanoVLM](./HF/nanoVLM/) | VLM |

## Datasets: 
| Name | Description | Path | Usecase |
|---------|-------------|------|------|
| MyST | Children's Conversational Speech | [📁 MyST](./Data/MyST/)  | ASR, TTS |
