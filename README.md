# Journey-to-Multimodality
This repository chronicles the ongoing exploration of multimodal AI and Transformer models, curating summaries of groundbreaking research that integrates various sensory inputs such as vision, sound, and language into cohesive AI systems. It showcases some of the coolest and most exciting work happening in the field, highlighting cutting-edge advancements that blend multiple sensory modalities to expand the capabilities of AI. Additionally, this repository includes my personal notes and insights, making it easier to access and understand the latest research. It‚Äôs a journey into the future of AI perception and cognition, offering a curated resource for those eager to explore how AI is evolving to process and interact with the world in increasingly human-like ways.


## Table of Contents

| Project | Description | Link | TASK |
|---------|-------------|----------|----------------|
| VALLR | Visual ASR Language Model for Lip Reading | [üìÅ VALLR](./VALLR/) | Visual Speech Recognition |
| AVFormer | Injecting Vision into Frozen Speech Models for Zero-Shot AV-ASR | [üìÅ AVFormer](./AVFormer/) | Audio Video Speech Recognition | 
| HTR-VT | Handwritten Text Recognition with Vision Transformer (uses CTC) | [üìÅ HTR-VT](./HTR-VT/) | Handwritten Text Recogntion | 
| AST | Audio Spectrogram Transformer| [üìÅ AST](./AST/) | Audio Classification | 
| LAVIS | A Library for Language-Vision Intelligence| [üìÅ LAVIS](./LAVIS/) | Language Vision - CLIP, BLIP, X-InstructBLIP, etc.| 
